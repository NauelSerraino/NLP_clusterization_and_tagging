{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import wordnet\n",
    "# from cluster_separation import cluster_separator_from_txt_file, read_cluster_file, create_clusters\n",
    "import os\n",
    "import re\n",
    "import importlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clusters(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    clusters = []\n",
    "    for line in lines:\n",
    "        cluster = line.strip().replace('\"', \"\").replace('[', \"\").replace(']', \"\").replace(\"'\", \"\").split(',')\n",
    "        if \"Cluster\" not in cluster[0]:\n",
    "            cluster = [word.strip() for word in cluster] \n",
    "            clusters.append(cluster)\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_labels_to_file(labels, path_to_file):\n",
    "    with open(path_to_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    current_cluster = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.startswith(\"Cluster \"):\n",
    "            current_cluster += 1\n",
    "            lines[i] = f\"{line.strip()} ({labels[current_cluster-1]})\\n\"\n",
    "\n",
    "    with open(path_to_file, 'w') as f:\n",
    "        f.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_PATH = r\"C:\\Users\\Nauel\\Desktop\\Lavoro UNI\"\n",
    "n_clusters = [5, 8, 10, 15, 20, 80, 100, 125, 150, 175, 200, 250, 300]\n",
    "PATH_INPUT = os.path.join(HOME_PATH, \"cluster_loop_clean\", \"definitions\", f\"Clusters con {n_cluster} gruppi & definizioni.txt\")\n",
    "PATH_OUTPUT = os.path.join(HOME_PATH, \"cluster_loop_clean\", \"labels\", f\"Clusters con {n_cluster} gruppi & definizioni.txt\")\n",
    "\n",
    "clusters = create_clusters(PATH_INPUT)\n",
    "\n",
    "# with open('most_common_hypernyms.txt', 'r') as file:\n",
    "#     predefined_labels = [line.strip() for line in file]\n",
    "#custom lables  \n",
    "predefined_lables = [\"family\", \"sport\", \"art\", \"manual labor\", \"intellectual labor/profession\", \"domestic work\", \"health work\", \"school\", \"free time/hobbies\",  \"leadership\", \"subordinate role\", \"parenthood\", \"home\", \"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['family',\n",
       " 'sport',\n",
       " 'art',\n",
       " 'manual labor',\n",
       " 'intellectual labor/profession',\n",
       " 'domestic work',\n",
       " 'health work',\n",
       " 'school',\n",
       " 'free time/hobbies',\n",
       " 'leadership',\n",
       " 'subordinate role',\n",
       " 'parenthood',\n",
       " 'home',\n",
       " 'neutral']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predefined_lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nauel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:588: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n",
      "c:\\Users\\Nauel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:588: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n",
      "c:\\Users\\Nauel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:588: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n",
      "c:\\Users\\Nauel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:588: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n",
      "c:\\Users\\Nauel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:588: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n",
      "c:\\Users\\Nauel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:588: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n",
      "c:\\Users\\Nauel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:588: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n",
      "c:\\Users\\Nauel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:588: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n",
      "c:\\Users\\Nauel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:588: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n",
      "c:\\Users\\Nauel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:588: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n",
      "c:\\Users\\Nauel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:588: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n",
      "c:\\Users\\Nauel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:588: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n",
      "c:\\Users\\Nauel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:588: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def read_clusters_from_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    clusters = []\n",
    "    for line in lines:\n",
    "        cluster = line.strip().replace('\"', \"\").replace('[', \"\").replace(']', \"\").replace(\"'\", \"\").split(',')\n",
    "        if \"Cluster\" not in cluster[0]:\n",
    "            cluster = [word.strip() for word in cluster] \n",
    "            clusters.append(cluster)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "def write_labels_to_file(labels, input_path, output_path):\n",
    "    with open(input_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    current_cluster = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.startswith(\"Cluster \"):\n",
    "            current_cluster += 1\n",
    "            lines[i] = f\"{line.strip()} ({labels[current_cluster-1]})\\n\"\n",
    "\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.writelines(lines)\n",
    "\n",
    "# Set the paths\n",
    "HOME_PATH = r\"C:\\Users\\Nauel\\Desktop\\Lavoro UNI\"\n",
    "n_clusters = [5, 8, 10, 15, 20, 80, 100, 125, 150, 175, 200, 250, 300]\n",
    "\n",
    "for n_cluster in n_clusters:\n",
    "    input_file = os.path.join(HOME_PATH, \"cluster_loop_clean\", \"definitions\", f\"Clusters con {n_cluster} gruppi & definizioni.txt\")\n",
    "    output_file = os.path.join(HOME_PATH, \"cluster_loop_clean\", \"labels_custom\", f\"Clusters con {n_cluster} gruppi & definizioni.txt\")\n",
    "\n",
    "    # Read clusters from the input file\n",
    "    clusters = read_clusters_from_file(input_file)\n",
    "\n",
    "    # Extract labels for the clusters\n",
    "    cluster_labels = compute_cluster_labels(clusters, predefined_labels)\n",
    "\n",
    "    # Write the labels to the output file\n",
    "    write_labels_to_file(cluster_labels, input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nauel\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:588: FutureWarning: `cached_download` is the legacy way to download files from the HF hub, please consider upgrading to `hf_hub_download`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['businessperson', 'associate', 'skilled_worker', 'clergyman', 'associate', 'businessperson', 'skilled_worker', 'worker', 'professional', 'businessperson', 'performer', 'worker', 'merchant', 'intellectual', 'communicator', 'communicator', 'parent', 'medical_practitioner', 'communicator', 'employee', 'administrator', 'communicator', 'skilled_worker', 'sexual_intercourse', 'athlete', 'clergyman', 'worker', 'communicator', 'skilled_worker', 'creator', 'worker', 'skilled_worker', 'employee', 'scientist', 'sexual_intercourse', 'performer', 'worker', 'businessperson', 'worker', 'machine', 'medical_practitioner', 'individual', 'communicator', 'businessperson', 'parent', 'parent', 'educator', 'sexual_intercourse', 'artist', 'musician', 'clergyman', 'athlete', 'parent', 'clergyman', 'artist', 'medical_practitioner', 'intellectual', 'clergyman', 'craftsman', 'athlete', 'craftsman', 'artist', 'parent', 'communicator', 'communicator', 'worker', 'parent', 'athlete', 'communicator', 'businessperson', 'creator', 'friend', 'doctor', 'performer', 'communicator', 'individual', 'medical_practitioner', 'businessperson', 'athlete', 'craftsman']\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_cluster_labels(clusters, predefined_labels):\n",
    "    model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "    \n",
    "    # Convert clusters to embeddings\n",
    "    cluster_texts = [' '.join(cluster) for cluster in clusters]\n",
    "    cluster_embeddings = convert_clusters_to_embeddings(cluster_texts, model)\n",
    "    \n",
    "    # Convert predefined labels to embeddings\n",
    "    predefined_labels_texts = [' '.join(label.split('_')) for label in predefined_labels]\n",
    "    predefined_labels_embeddings = convert_clusters_to_embeddings(predefined_labels_texts, model)\n",
    "    \n",
    "    # Compute similarity scores\n",
    "    similarity_scores = compute_similarity_scores(cluster_embeddings, predefined_labels_embeddings)\n",
    "    \n",
    "    # Find the most similar label for each cluster\n",
    "    cluster_labels = [predefined_labels[np.argmax(scores)] for scores in similarity_scores]\n",
    "    \n",
    "    return cluster_labels\n",
    "\n",
    "def convert_clusters_to_embeddings(cluster_texts, model):\n",
    "    cluster_embeddings = model.encode(cluster_texts, convert_to_tensor=True)\n",
    "    return cluster_embeddings\n",
    "\n",
    "def compute_similarity_scores(cluster_embeddings, predefined_labels_embeddings):\n",
    "    similarity_scores = cosine_similarity(cluster_embeddings, predefined_labels_embeddings)\n",
    "    return similarity_scores\n",
    "\n",
    "cluster_labels = compute_cluster_labels(clusters, predefined_labels)\n",
    "print(cluster_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_cluster in n_clusters:\n",
    "    PATH_INPUT = os.path.join(HOME_PATH, \"cluster_loop_clean\", \"definitions\", f\"Clusters con {n_cluster} gruppi & definizioni.txt\")\n",
    "    PATH_OUTPUT = os.path.join(HOME_PATH, \"cluster_loop_clean\", \"labels\", f\"Clusters con {n_cluster} gruppi & definizioni.txt\")\n",
    "    clusters = create_clusters(PATH_INPUT)\n",
    "    labels = [get_most_similar_labels(cluster, predefined_labels) for cluster in clusters]\n",
    "    write_labels_to_file(labels, PATH_OUTPUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def create_clusters(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    clusters = []\n",
    "    for line in lines:\n",
    "        cluster = line.strip().replace('\"', \"\").replace('[', \"\").replace(']', \"\").replace(\"'\", \"\").split(',')\n",
    "        if \"Cluster\" not in cluster[0]:\n",
    "            cluster = [word.strip() for word in cluster]\n",
    "            clusters.append(cluster)\n",
    "\n",
    "    return clusters\n",
    "\n",
    "def write_labels_to_file(labels, path_to_file):\n",
    "    with open(path_to_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    current_cluster = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.startswith(\"Cluster \"):\n",
    "            current_cluster += 1\n",
    "            lines[i] = f\"{line.strip()} ({labels[current_cluster-1]})\\n\"\n",
    "\n",
    "    with open(path_to_file, 'w') as f:\n",
    "        f.writelines(lines)\n",
    "\n",
    "def associate_clusters_with_labels(clusters, predefined_labels):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    def calculate_similarity(sentence1, sentence2):\n",
    "        inputs = tokenizer.encode_plus(sentence1, sentence2, add_special_tokens=True, return_tensors='pt')\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            sentence_embeddings = torch.mean(outputs.last_hidden_state, dim=1)\n",
    "        similarity = 1 - cosine(sentence_embeddings[0].cpu(), torch.mean(sentence_embeddings, dim=0).cpu())\n",
    "        return similarity\n",
    "\n",
    "\n",
    "\n",
    "    cluster_labels = []\n",
    "    for cluster in clusters:\n",
    "        max_similarity = 0\n",
    "        cluster_label = None\n",
    "        for label in predefined_labels:\n",
    "            similarity = calculate_similarity(' '.join(cluster), label)\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "                cluster_label = label\n",
    "        cluster_labels.append(cluster_label)\n",
    "\n",
    "    return cluster_labels\n",
    "\n",
    "HOME_PATH = r\"C:\\Users\\Nauel\\Desktop\\Lavoro UNI\"\n",
    "n_clusters = [5, 8, 10, 15, 20, 80, 100, 125, 150, 175, 200, 250, 300]\n",
    "\n",
    "for n_cluster in n_clusters:\n",
    "    PATH_INPUT = os.path.join(HOME_PATH, \"cluster_loop_clean\", \"definitions\", f\"Clusters con {n_cluster} gruppi & definizioni.txt\")\n",
    "    PATH_OUTPUT = os.path.join(HOME_PATH, \"cluster_loop_clean\", \"labels\", f\"Clusters con {n_cluster} gruppi & definizioni.txt\")\n",
    "    clusters = create_clusters(PATH_INPUT)\n",
    "    labels = associate_clusters_with_labels(clusters, predefined_labels)\n",
    "    write_labels_to_file(labels, PATH_OUTPUT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f46ce1e9f86266199c0d76baff9f68961607da6dc69ecc7d2e8972faaca65561"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
